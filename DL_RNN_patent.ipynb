{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_RNN_patent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415uhIj6DA7G"
      },
      "source": [
        "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMGCqb408mLS"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDa_rYJZ-WER",
        "outputId": "b34c0837-06eb-4066-ecf3-f506720a89d4"
      },
      "source": [
        "%%writefile utils.py\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from itertools import chain\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "\n",
        "RANDOM_STATE = 50\n",
        "TRAIN_FRACTION = 0.7\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "    \"\"\"Retrieve a Keras model and embeddings\"\"\"\n",
        "    model = load_model(f'../models/{model_name}.h5')\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    word_idx = []\n",
        "    with open(f'../data/training-rnn.json', 'rb') as f:\n",
        "        for l in f:\n",
        "            word_idx.append(json.loads(l))\n",
        "        \n",
        "    word_idx = word_idx[0]\n",
        "    word_idx['UNK'] = 0\n",
        "    idx_word = {index: word for word, index in word_idx.items()}\n",
        "    return model, embeddings, word_idx, idx_word\n",
        "\n",
        "def get_embeddings(model):\n",
        "    \"\"\"Retrieve the embeddings in a model\"\"\"\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    return embeddings\n",
        "    \n",
        "def find_closest(query, embedding_matrix, word_idx, idx_word, n = 10):\n",
        "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
        "    \n",
        "    idx = word_idx.get(query, None)\n",
        "    # Handle case where query is not in vocab\n",
        "    if idx is None:\n",
        "        print(f'{query} not found in vocab.')\n",
        "        return\n",
        "    else:\n",
        "        vec = embedding_matrix[idx]\n",
        "        # Handle case where word doesn't have an embedding\n",
        "        if np.all(vec == 0):\n",
        "            print(f'{query} has no pre-trained embedding.')\n",
        "            return\n",
        "        else:\n",
        "            # Calculate distance between vector and all others\n",
        "            dists = np.dot(embedding_matrix, vec)\n",
        "            \n",
        "            # Sort indexes in reverse order\n",
        "            idxs = np.argsort(dists)[::-1][:n]\n",
        "            sorted_dists = dists[idxs]\n",
        "            closest = [idx_word[i] for i in idxs]\n",
        "            \n",
        "    print(f'Query: {query}\\n')\n",
        "    # Print out the word and cosine distances\n",
        "    for word, dist in zip(closest, sorted_dists):\n",
        "        print(f'Word: {word:15} Cosine Similarity: {round(dist, 4)}')\n",
        "        \n",
        "def format_sequence(s):\n",
        "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
        "    \n",
        "    # Add spaces around punctuation\n",
        "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
        "    \n",
        "    # Remove references to figures\n",
        "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
        "    \n",
        "    # Remove double spaces\n",
        "    s = re.sub(r'\\s\\s', ' ', s)\n",
        "    return s\n",
        "\n",
        "def remove_spaces(s):\n",
        "    \"\"\"Remove spaces around punctuation\"\"\"\n",
        "    s = re.sub(r'\\s+([.,;?])', r'\\1', s)\n",
        "    \n",
        "    return s\n",
        "\n",
        "\n",
        "def get_data(file, filters='!\"%;[\\\\]^_`{|}~\\t\\n', training_len=50,\n",
        "             lower=False):\n",
        "    \"\"\"Retrieve formatted training and validation data from a file\"\"\"\n",
        "    \n",
        "    data = pd.read_csv(file, parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])\n",
        "    abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]\n",
        "    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n",
        "        abstracts, training_len, lower, filters)\n",
        "    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
        "    training_dict = {'X_train': X_train, 'X_valid': X_valid, \n",
        "                     'y_train': y_train, 'y_valid': y_valid}\n",
        "    return training_dict, word_idx, idx_word, sequences\n",
        "\n",
        "def create_train_valid(features,\n",
        "                       labels,\n",
        "                       num_words,\n",
        "                       train_fraction=0.7):\n",
        "    \"\"\"Create training and validation features and labels.\"\"\"\n",
        "    \n",
        "    # Randomly shuffle features and labels\n",
        "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Decide on number of samples for training\n",
        "    train_end = int(train_fraction * len(labels))\n",
        "\n",
        "    train_features = np.array(features[:train_end])\n",
        "    valid_features = np.array(features[train_end:])\n",
        "\n",
        "    train_labels = labels[:train_end]\n",
        "    valid_labels = labels[train_end:]\n",
        "\n",
        "    # Convert to arrays\n",
        "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "\n",
        "    # Using int8 for memory savings\n",
        "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
        "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
        "\n",
        "    # One hot encoding of labels\n",
        "    for example_index, word_index in enumerate(train_labels):\n",
        "        y_train[example_index, word_index] = 1\n",
        "\n",
        "    for example_index, word_index in enumerate(valid_labels):\n",
        "        y_valid[example_index, word_index] = 1\n",
        "\n",
        "    # Memory management\n",
        "    import gc\n",
        "    gc.enable()\n",
        "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
        "    gc.collect()\n",
        "\n",
        "    return X_train, X_valid, y_train, y_valid\n",
        "\n",
        "def make_sequences(texts, training_length = 50,\n",
        "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "    \n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "    \n",
        "    print(f'There are {num_words} unique words.')\n",
        "    \n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    \n",
        "    # Limit to sequences with more than training length tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n",
        "    \n",
        "    new_texts = []\n",
        "    new_sequences = []\n",
        "    \n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "        new_sequences.append(sequences[i])\n",
        "        \n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "        \n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length: i + 1]\n",
        "            \n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "    \n",
        "    print(f'There are {len(features)} sequences.')\n",
        "    \n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n",
        "\n",
        "def generate_output(model,\n",
        "                    sequences,\n",
        "                    idx_word,\n",
        "                    seed_length=50,\n",
        "                    new_words=50,\n",
        "                    diversity=1,\n",
        "                    return_output=False,\n",
        "                    n_gen=1):\n",
        "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
        "\n",
        "    # Choose a random sequence\n",
        "    seq = random.choice(sequences)\n",
        "\n",
        "    # Choose a random starting point\n",
        "    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
        "    # Ending index for seed\n",
        "    end_idx = seed_idx + seed_length\n",
        "\n",
        "    gen_list = []\n",
        "\n",
        "    for n in range(n_gen):\n",
        "        # Extract the seed sequence\n",
        "        seed = seq[seed_idx:end_idx]\n",
        "        original_sequence = [idx_word[i] for i in seed]\n",
        "        generated = seed[:] + ['#']\n",
        "\n",
        "        # Find the actual entire sequence\n",
        "        actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
        "\n",
        "        # Keep adding new words\n",
        "        for i in range(new_words):\n",
        "\n",
        "            # Make a prediction from the seed\n",
        "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
        "                np.float64)\n",
        "\n",
        "            # Diversify\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "\n",
        "            # Softmax\n",
        "            preds = exp_preds / sum(exp_preds)\n",
        "\n",
        "            # Choose the next word\n",
        "            probas = np.random.multinomial(1, preds, 1)[0]\n",
        "\n",
        "            next_idx = np.argmax(probas)\n",
        "\n",
        "            # New seed adds on old word\n",
        "            #             seed = seed[1:] + [next_idx]\n",
        "            seed += [next_idx]\n",
        "            generated.append(next_idx)\n",
        "\n",
        "        # Showing generated and actual abstract\n",
        "        n = []\n",
        "\n",
        "        for i in generated:\n",
        "            n.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "        gen_list.append(n)\n",
        "\n",
        "    a = []\n",
        "\n",
        "    for i in actual:\n",
        "        a.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "    a = a[seed_length:]\n",
        "\n",
        "    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
        "\n",
        "    if return_output:\n",
        "        return original_sequence, gen_list, a\n",
        "\n",
        "    # HTML formatting\n",
        "    seed_html = ''\n",
        "    seed_html = addContent(seed_html, header(\n",
        "        'Seed Sequence', color='darkblue'))\n",
        "    seed_html = addContent(seed_html,\n",
        "                           box(remove_spaces(' '.join(original_sequence))))\n",
        "\n",
        "    gen_html = ''\n",
        "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
        "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n",
        "\n",
        "    a_html = ''\n",
        "    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n",
        "    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n",
        "\n",
        "    return seed_html, gen_html, a_html\n",
        "\n",
        "\n",
        "\n",
        "def header(text, color = 'black', gen_text = None):\n",
        "    if gen_text:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><p><center>' + str(\n",
        "        text) + '<span style=\"color: red\">' + str(gen_text) + '</center></p></h1>'\n",
        "    else:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><center>' + str(\n",
        "            text) + '</center></h1>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def box(text, gen_text=None):\n",
        "    if gen_text:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>' + str(\n",
        "            text) +'<span style=\"color: red\">' + str(gen_text) + '</p></div>'\n",
        "\n",
        "    else:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + str(\n",
        "            text) + '</div>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def addContent(old_html, raw_html):\n",
        "    old_html += raw_html\n",
        "    return old_html\n",
        "\n",
        "def seed_sequence(model, s, word_idx, idx_word, \n",
        "                  diversity = 0.75, num_words = 50):\n",
        "    \"\"\"Generate output starting from a seed sequence.\"\"\"\n",
        "    # Original formated text\n",
        "    start = format_sequence(s).split()\n",
        "    gen = []\n",
        "    s = start[:]\n",
        "    # Generate output\n",
        "    for _ in range(num_words):\n",
        "        # Conver to arry\n",
        "        x = np.array([word_idx.get(word, 0) for word in s]).reshape((1, -1))\n",
        "\n",
        "        # Make predictions\n",
        "        preds = model.predict(x)[0].astype(float)\n",
        "\n",
        "        # Diversify\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        # Softmax\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        # Pick next index\n",
        "        next_idx = np.argmax(np.random.multinomial(1, preds, size = 1))\n",
        "        s.append(idx_word[next_idx])\n",
        "        gen.append(idx_word[next_idx])\n",
        "    \n",
        "    # Formatting in html\n",
        "    start = remove_spaces(' '.join(start)) + ' '\n",
        "    gen = remove_spaces(' '.join(gen)) \n",
        "    html = ''\n",
        "    html = addContent(html, header('Input Seed ', color = 'black', gen_text = 'Network Output'))\n",
        "    html = addContent(html, box(start, gen))\n",
        "    return html\n",
        "\n",
        "def guess_human(model, sequences, idx_word, seed_length=50):\n",
        "    \"\"\"Produce 2 RNN sequences and play game to compare to actaul.\n",
        "       Diversity is randomly set between 0.5 and 1.25\"\"\"\n",
        "    \n",
        "    new_words = np.random.randint(10, 50)\n",
        "    diversity = np.random.uniform(0.5, 1.25)\n",
        "    sequence, gen_list, actual = generate_output(model, sequences, idx_word, seed_length, new_words,\n",
        "                                                 diversity=diversity, return_output=True, n_gen = 2)\n",
        "    gen_0, gen_1 = gen_list\n",
        "    \n",
        "    output = {'sequence': remove_spaces(' '.join(sequence)),\n",
        "              'computer0': remove_spaces(' '.join(gen_0)),\n",
        "              'computer1': remove_spaces(' '.join(gen_1)),\n",
        "              'human': remove_spaces(' '.join(actual))}\n",
        "    \n",
        "    print(f\"Seed Sequence: {output['sequence']}\\n\")\n",
        "    \n",
        "    choices = ['human', 'computer0', 'computer1']\n",
        "          \n",
        "    selected = []\n",
        "    i = 0\n",
        "    while len(selected) < 3:\n",
        "        choice = random.choice(choices)\n",
        "        selected.append(choice)\n",
        "        print(f'\\nOption {i + 1} {output[choice]}')\n",
        "        choices.remove(selected[-1])\n",
        "        i += 1\n",
        "    \n",
        "    print('\\n')\n",
        "    guess = int(input('Enter option you think is human (1-3): ')) - 1\n",
        "    print('\\n')\n",
        "    \n",
        "    if guess == np.where(np.array(selected) == 'human')[0][0]:\n",
        "        print('*' * 3 + 'Correct' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Ordering: ', selected)\n",
        "    else:\n",
        "        print('*' * 3 + 'Incorrect' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Correct Ordering: ', selected)\n",
        "          \n",
        "    print('Diversity', round(diversity, 2))\n",
        "    \n",
        "def make_sequences_new(texts,\n",
        "                   training_length=50,\n",
        "                   lower=True,\n",
        "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "\n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Limit to sequences with more than (training length + 20) tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [\n",
        "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
        "    ]\n",
        "\n",
        "    new_texts = []\n",
        "\n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "    \n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    # Refit on long texts\n",
        "    tokenizer.fit_on_texts(new_texts)\n",
        "    new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "\n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length:i + 1]\n",
        "\n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "\n",
        "    print(f'There are {len(features)} training sequences.')\n",
        "\n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5kldVzc8-5q"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from IPython.display import HTML\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category = UserWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "YD7o-Da_8-8a",
        "outputId": "2a3e3862-0d6f-4c9d-e87f-2a2432b5b32d"
      },
      "source": [
        "data = pd.read_csv('/content/neural_network_patent_query.csv')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patent_abstract</th>\n",
              "      <th>patent_date</th>\n",
              "      <th>patent_number</th>\n",
              "      <th>patent_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
              "      <td>1996-07-09</td>\n",
              "      <td>5535303</td>\n",
              "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\" This invention is a novel high-speed neural ...</td>\n",
              "      <td>1993-10-19</td>\n",
              "      <td>5255349</td>\n",
              "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>An optical information processor for use as a ...</td>\n",
              "      <td>1995-01-17</td>\n",
              "      <td>5383042</td>\n",
              "      <td>3 layer liquid crystal neural network with out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2001-01-02</td>\n",
              "      <td>6169981</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2003-06-17</td>\n",
              "      <td>6581048</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     patent_abstract  ...                                       patent_title\n",
              "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  ...        \"\"\"Barometer\"\" neuron for a neural network\"\n",
              "1  \" This invention is a novel high-speed neural ...  ...  \"Electronic neural network for solving \"\"trave...\n",
              "2  An optical information processor for use as a ...  ...  3 layer liquid crystal neural network with out...\n",
              "3  A method and system for intelligent control of...  ...  3-brain architecture for an intelligent decisi...\n",
              "4  A method and system for intelligent control of...  ...  3-brain architecture for an intelligent decisi...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMahheId8-_M",
        "outputId": "21048905-f9ad-4286-8217-932cf3985fe8"
      },
      "source": [
        "training_dict, word_idx, idx_word, sequences = get_data('/content/neural_network_patent_query.csv', training_len = 50)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 16192 unique words.\n",
            "There are 318563 sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "503kKp2n8_B1",
        "outputId": "06cb3729-6ae1-43bc-844d-ce4d46d68646"
      },
      "source": [
        "training_dict['X_train'][:2]\n",
        "training_dict['y_train'][:2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  117,     7,   141,   277,     4,    18,    81,   110,    10,\n",
              "          219,    29,     1,   952,  2453,    19,     5,     6,     1,\n",
              "          117,    10,   182,  2166,    21,     1,    81,   178,     4,\n",
              "           13,   117,   894,    14,  6163,     7,   302,     1,     9,\n",
              "            8,    29,    33,    23,    74,   428,     7,   692,     1,\n",
              "           81,   183,     4,    13,   117],\n",
              "       [    6,    41,     2,    87,     3,  1340,    79,     7,     1,\n",
              "          409,   543,    22,   484,     6,     2,  2113,   728,    24,\n",
              "            1,   178,     3,     1,  1820,    55,    14, 13942,  7240,\n",
              "          244,     5,    14, 13943,  7240,   244,     5,     2,  2113,\n",
              "         7240,   244,     5,     2,    38,  9292,   244,     2,    49,\n",
              "         9292,   244,    14,    22, 13944]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdIMO7ye8_Ep",
        "outputId": "71efd2e4-a5d8-48ac-b0ac-bf4b85d0573a"
      },
      "source": [
        "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
        "    text = []\n",
        "    for idx in sequence:\n",
        "        text.append(idx_word[idx])\n",
        "        \n",
        "    print('Features: ' + ' '.join(text) + '\\n')\n",
        "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: user to provide samples . A recognition operation is performed on the user's handwritten input , and the user is not satisfied with the recognition result . The user selects an option to train the neural network on one or more characters to improve the recognition results . The user\n",
            "\n",
            "Label: is\n",
            "\n",
            "Features: and includes a number of amplifiers corresponding to the N bit output sum and a carry generation from the result of the adding process an augend input-synapse group , an addend input-synapse group , a carry input-synapse group , a first bias-synapse group a second bias-synapse group an output feedback-synapse\n",
            "\n",
            "Label: group\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuOFw88g8_HT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OSLgaqr8_J-",
        "outputId": "01e708ad-ea5f-4a39-b2f1-3a51a5f5da4b"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=len(word_idx) + 1, output_dim=100, weights=None, trainable=True)) # Embedding layer\n",
        "model.add(LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))   # Recurrent layer\n",
        "model.add(Dense(64, activation='relu'))   # Fully connected layer\n",
        "model.add(Dropout(0.5))   # Dropout for regularization\n",
        "model.add(Dense(len(word_idx) + 1, activation='softmax'))   # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         1619200   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                42240     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16192)             1052480   \n",
            "=================================================================\n",
            "Total params: 2,718,080\n",
            "Trainable params: 2,718,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFw6XQRg8_Mw",
        "outputId": "509cc415-e04d-4c78-f26d-facc77349412"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load in model and demonstrate training\n",
        "model = load_model('/content/train-embeddings-rnn.h5')\n",
        "h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs = 5, batch_size = 2048, \n",
        "          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n",
        "          verbose = 1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "109/109 [==============================] - 319s 3s/step - loss: 3.7546 - accuracy: 0.2955 - val_loss: 5.1299 - val_accuracy: 0.2675\n",
            "Epoch 2/5\n",
            "109/109 [==============================] - 324s 3s/step - loss: 3.7370 - accuracy: 0.2966 - val_loss: 5.1525 - val_accuracy: 0.2684\n",
            "Epoch 3/5\n",
            "109/109 [==============================] - 322s 3s/step - loss: 3.7242 - accuracy: 0.2966 - val_loss: 5.1706 - val_accuracy: 0.2686\n",
            "Epoch 4/5\n",
            "109/109 [==============================] - 321s 3s/step - loss: 3.7113 - accuracy: 0.2991 - val_loss: 5.1649 - val_accuracy: 0.2689\n",
            "Epoch 5/5\n",
            "109/109 [==============================] - 315s 3s/step - loss: 3.7004 - accuracy: 0.2988 - val_loss: 5.1763 - val_accuracy: 0.2701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWy0FPG78_Pb",
        "outputId": "7d838f81-17f9-44a5-d44d-3a58af77aa12"
      },
      "source": [
        "model = load_model('/content/train-embeddings-rnn.h5')\n",
        "print('Model Performance: Log Loss and Accuracy on training data')\n",
        "model.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n",
        "\n",
        "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
        "model.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance: Log Loss and Accuracy on training data\n",
            "109/109 [==============================] - 88s 801ms/step - loss: 3.2897 - accuracy: 0.3384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.289726495742798, 0.3384440839290619]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance: Log Loss and Accuracy on validation data\n",
            "47/47 [==============================] - 37s 785ms/step - loss: 5.1321 - accuracy: 0.2672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.132135391235352, 0.2671891450881958]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m53xAkDA8_VB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "8697aa0d-26a1-4062-ef07-799815a78c62"
      },
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 50, new_words = 30, diversity = 0.75):\n",
        "    HTML(i)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">an engine representative of a particular flight condition and generates a set of estimated engine parameters representative of the model. The system further comprises a comparator for comparing the set of estimated engine parameters to a set of measured engine parameters for generating a set of residuals and an</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > output for an output of the neural network, and a second target set of coefficients associated with the output parameters. The neural network may be trained with the</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > artificial neural network module to be trained and to be used in an implementation configuration after training has been completed. The artificial neural network receives the set of residuals</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2UFDKnX8_X2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "f4ec9bcc-ecd7-42d6-c168-6e559eefed26"
      },
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 30, new_words = 30, diversity = 1.5):\n",
        "    HTML(i)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">value and variance of the vehicle driving parameters and the weighted total sum of the road traffic condition parameters to nonlinear conversion. Then, it variably controls the operating</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > station users into the motor action group. After the estimated locations associated with the process, other measured parts are information which has been trained.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > characteristic of a vehicle-mounted apparatus such as a rear-wheel steering controlling unit in accordance with the output parameter, thereby variably controlling the vehicle running characteristic.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgIFoQPN8_ag",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "c0b54ac0-1428-447d-ee01-7a45996850d1"
      },
      "source": [
        "s = 'This patent provides a basis for using a recurrent neural network to '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>This patent provides a basis for using a recurrent neural network to <span style=\"color: red\">many aspect and the plant. The query is used for use with a neural network, and a neural</p></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJVbecKH8_dW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b262f865-0b80-4315-fb68-3be2b814f614"
      },
      "source": [
        "s = 'The cell state is passed along from one time step to another allowing the '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>The cell state is passed along from one time step to another allowing the <span style=\"color: red\">output corresponding to the level to the array of the neural network using a plurality of time distances by a</p></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3pfGQjz8_f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9a7481-46f6-47cb-d709-bb8f74740132"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Sequence: for contiguous portions of digital images are disclosed. For one embodiment, as an example, parameters of a neural network may be developed to generate object labels for digital images. The developed parameters may be transferred to a neural network utilized to generate signal sample value levels\n",
            "\n",
            "\n",
            "Option 1 < --- > corresponding to preference indices for contiguous portions of digital images.\n",
            "\n",
            "Option 2 < --- > outputted with a source-to-image direction of the predetermined distance or a\n",
            "\n",
            "Option 3 < --- > responsive to output values comprising one set of hypothetical weight computing\n",
            "\n",
            "\n",
            "Enter option you think is human (1-3): 2\n",
            "\n",
            "\n",
            "***Incorrect***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Correct Ordering:  ['human', 'computer1', 'computer0']\n",
            "Diversity 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQswppjZ8_ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3863cb17-7964-41bc-9f50-e501cefac5a6"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Sequence: the arrangement in order to deliver/receive, via a single bus, permuted data words which correspond either to a row or to a column of the matrix. Each data cell is connected to the single bus via series-connected switches which are associated with a respective addressing mode,\n",
            "\n",
            "\n",
            "Option 1 < --- > a edge holding the first layer of the groups of the input neurons. The neuron interconnects an output layer to the symmetric neurons of the neuron, in the neuron for a\n",
            "\n",
            "Option 2 < --- > the second input neuron in a first core unit generates or equal of a threshold. The weighting elements may be interconnected to generate a part of the first frame stored in a\n",
            "\n",
            "Option 3 < --- > the switches which address a same word of a same mode being directly controlled by a same selection signal. Circulation members enable the original order of the data on the bus to\n",
            "\n",
            "\n",
            "Enter option you think is human (1-3): 1\n",
            "\n",
            "\n",
            "***Incorrect***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Correct Ordering:  ['computer1', 'computer0', 'human']\n",
            "Diversity 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUAmxok08_lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6918b7b8-1cc4-4081-f704-b0f38f068237"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Sequence: A borehole logging tool having a pair of spaced-apart detectors records intensity signals representing the die-away of nuclear radiation in a subsurface formation. Weighted moments of the intensity signals as well as of a model are produced. Corresponding weighted intensity and model moments are equated and simultaneously solved\n",
            "\n",
            "\n",
            "Option 1 < --- > to provide inputs for minimizing the incident value). The PIO amount of this power waveforms is\n",
            "\n",
            "Option 2 < --- >, thereby a) the implantee indicated. On a central control system having compared with combustor parameter\n",
            "\n",
            "Option 3 < --- > to obtain values for a borehole decay constant, a formation decay constant, and a formation-to-borehole\n",
            "\n",
            "\n",
            "Enter option you think is human (1-3): 3\n",
            "\n",
            "\n",
            "***Correct***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ordering:  ['computer1', 'computer0', 'human']\n",
            "Diversity 1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQSbozpc8_oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225766be-dae2-40dc-c5cd-6e62ee2b80e3"
      },
      "source": [
        "embeddings = get_embeddings(model)\n",
        "embeddings.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16192, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNsaAbYj8_rH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b626aeef-db6c-40be-8577-c22c976cda9e"
      },
      "source": [
        "find_closest('network', embeddings, word_idx, idx_word)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: network\n",
            "\n",
            "Word: network         Cosine Similarity: 1.0\n",
            "Word: channel         Cosine Similarity: 0.7754999995231628\n",
            "Word: networks        Cosine Similarity: 0.7745000123977661\n",
            "Word: system          Cosine Similarity: 0.7559999823570251\n",
            "Word: program         Cosine Similarity: 0.7541999816894531\n",
            "Word: cable           Cosine Similarity: 0.7419999837875366\n",
            "Word: now             Cosine Similarity: 0.7297999858856201\n",
            "Word: programming     Cosine Similarity: 0.7179999947547913\n",
            "Word: web             Cosine Similarity: 0.7138000130653381\n",
            "Word: line            Cosine Similarity: 0.6915000081062317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93hJKZ_YCDpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb20f5c7-8a36-495c-e708-d14fe3711c4e"
      },
      "source": [
        "find_closest('data', embeddings, word_idx, idx_word)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: data\n",
            "\n",
            "Word: data            Cosine Similarity: 1.0\n",
            "Word: information     Cosine Similarity: 0.8185999989509583\n",
            "Word: numbers         Cosine Similarity: 0.683899998664856\n",
            "Word: database        Cosine Similarity: 0.6776000261306763\n",
            "Word: account         Cosine Similarity: 0.6575999855995178\n",
            "Word: report          Cosine Similarity: 0.6575999855995178\n",
            "Word: signals         Cosine Similarity: 0.6399999856948853\n",
            "Word: system          Cosine Similarity: 0.6377000212669373\n",
            "Word: statistics      Cosine Similarity: 0.6371999979019165\n",
            "Word: web             Cosine Similarity: 0.6359000205993652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmdN9XGrCDuM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvSEGg-KCD0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec239a7b-dfc3-4130-98c3-dbf605e4964e"
      },
      "source": [
        "import requests \n",
        "\n",
        "response = requests.get('https://google.com/') \n",
        "print(response) "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpblypvDCD4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjiL7ZB6CD9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba226ed-81ef-4cc3-efbe-a1cbd2ebbdb1"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) \n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYmOQvV3CEB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b92720c-9494-45ef-888f-2ae847f0b785"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 494, 32)           28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 98, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 92, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,315,937\n",
            "Trainable params: 1,315,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 66s 416ms/step - loss: 0.7076 - acc: 0.5363 - val_loss: 0.6816 - val_acc: 0.5878\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 65s 415ms/step - loss: 0.6587 - acc: 0.6758 - val_loss: 0.6536 - val_acc: 0.6834\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 65s 415ms/step - loss: 0.6010 - acc: 0.7699 - val_loss: 0.5727 - val_acc: 0.7650\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 65s 414ms/step - loss: 0.4844 - acc: 0.8188 - val_loss: 0.4590 - val_acc: 0.8158\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 65s 416ms/step - loss: 0.3863 - acc: 0.8529 - val_loss: 0.4451 - val_acc: 0.8420\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 65s 415ms/step - loss: 0.3310 - acc: 0.8783 - val_loss: 0.4275 - val_acc: 0.8518\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 65s 414ms/step - loss: 0.2919 - acc: 0.8957 - val_loss: 0.4344 - val_acc: 0.8612\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 65s 415ms/step - loss: 0.2618 - acc: 0.9088 - val_loss: 0.4480 - val_acc: 0.8666\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 65s 415ms/step - loss: 0.2393 - acc: 0.9186 - val_loss: 0.4757 - val_acc: 0.8640\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 65s 415ms/step - loss: 0.2184 - acc: 0.9258 - val_loss: 0.4868 - val_acc: 0.8720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWQE7QskCEGd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7MId5LiCEMC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_sjo5kCEP2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IAWDepA8_t3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}