{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_CNN+RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPgZmP5ruiFG"
      },
      "source": [
        "# Combining CNNs and RNNs to process long sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALHeMylbui-s"
      },
      "source": [
        "## jena data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IR6CV1ftKPt",
        "outputId": "2094ed1d-b7de-478a-e4f1-37101d01d88c"
      },
      "source": [
        "!unzip jena_climate.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  jena_climate.zip\n",
            "  inflating: jena_climate_2009_2016.csv  \n",
            "  inflating: __MACOSX/._jena_climate_2009_2016.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIeUC6V2tMC2",
        "outputId": "5137919c-f871-45be-d932-01b5db77e98a"
      },
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content'\n",
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
        "\n",
        "\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()\n",
        "\n",
        "\n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
            "420451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D4qvtAUuIYj"
      },
      "source": [
        "# Parsing the data\n",
        "import numpy as np\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(',')[1:]]\n",
        "    float_data[i, :] = values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-YEeHm0tmly"
      },
      "source": [
        "# Normalizing the data\n",
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_P_kK7btmi_"
      },
      "source": [
        "# Generator yielding timeseries samples and targets\n",
        "\n",
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGVr8woGtmgd"
      },
      "source": [
        "# Preparing the training, validation, and test generator\n",
        "\n",
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step,\n",
        "                      batch_size=batch_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX_FLa0rtmeA"
      },
      "source": [
        "# Preparing the training, validation, and test generator\n",
        "\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "                    \n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "val_steps = (300000 - 200001 - lookback)\n",
        "test_steps = (len(float_data) - 300001 - lookback)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EZeaBVZtmbX",
        "outputId": "a1182a31-02b6-4032-c194-f41775c44537"
      },
      "source": [
        "def evaluate_naive_method():\n",
        "    batch_maes = []\n",
        "    for step in range(val_steps):\n",
        "        samples, targets = next(val_gen)\n",
        "        preds = samples[:, -1, 1]\n",
        "        mae = np.mean(np.abs(preds - targets))\n",
        "        batch_maes.append(mae)\n",
        "    print(np.mean(batch_maes))\n",
        "evaluate_naive_method()     "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.28969941979609765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08pxPi5ht1SH"
      },
      "source": [
        "# Converting the MAE back to Celsius error\n",
        "celsius_mae = 0.29 * std[1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-v-5oXt5mB"
      },
      "source": [
        "# simple 1D convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgGavdXot1PZ",
        "outputId": "f9747b99-8b48-4348-ba59-b2c7bed78b32"
      },
      "source": [
        "# Training and evaluating a simple 1D convnet on the Jena data\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation='relu', input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 2006s 4s/step - loss: 0.4156 - val_loss: 0.4657\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 2023s 4s/step - loss: 0.3588 - val_loss: 0.4815\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 2037s 4s/step - loss: 0.3367 - val_loss: 0.4631\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 2055s 4s/step - loss: 0.3187 - val_loss: 0.4520\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 2101s 4s/step - loss: 0.3071 - val_loss: 0.4465\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 2105s 4s/step - loss: 0.2995 - val_loss: 0.4546\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 2060s 4s/step - loss: 0.2912 - val_loss: 0.4948\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 2061s 4s/step - loss: 0.2837 - val_loss: 0.4626\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 2063s 4s/step - loss: 0.2787 - val_loss: 0.4641\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 2034s 4s/step - loss: 0.2714 - val_loss: 0.4664\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 2029s 4s/step - loss: 0.2671 - val_loss: 0.4495\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - ETA: 0s - loss: 0.2642"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msz4vK42PqdR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss)+1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Tranining and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGm1JHOjt1Hn"
      },
      "source": [
        "# Preparing higher-resolution data generators for the Jena dataset\n",
        "step = 3\n",
        "lookback = 1440\n",
        "delay = 144\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2AQSUMtxvzn"
      },
      "source": [
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step)\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step)\n",
        "val_steps = (300000 - 200001 - lookback) // 128\n",
        "test_steps = (len(float_data) - 300001 - lookback) // 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJtjJlY4xyVR"
      },
      "source": [
        "# Model combining a 1D conventional base and a GRU layer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation='relu', input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlr_FhH3x21p"
      },
      "source": [
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator( train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                                epochs=20,\n",
        "                                validation_data=val_gen,\n",
        "                                validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD0_mtcBx2u-"
      },
      "source": [
        "epochs = range(1, len(loss)+1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Tranining and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKLTCNR8x2m3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}